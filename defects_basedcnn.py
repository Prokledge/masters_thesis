# -*- coding: utf-8 -*-
"""Defects-BasedCNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dftQhqF6WByehpDPjP5QLpnM9TnXykQ
"""

!pip install torch torchvision torchaudio transformers datasets

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
from datasets import load_dataset
import matplotlib.pyplot as plt

# Load the dataset and tokenizer
dataset = load_dataset("vm-prkl-hug/ama_electronics_labeled", split="train")
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenization function
def tokenize(batch):
    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)

# Apply tokenization
dataset = dataset.map(tokenize, batched=True)

# Split dataset using Hugging Face's built-in method
dataset = dataset.train_test_split(test_size=0.2, seed=42)

# Get the train and test sets
train_data = dataset['train']
test_data = dataset['test']

# Set format for train/test datasets
train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])


# Use DataCollatorWithPadding to handle dynamic padding within each batch
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Create DataLoaders
train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=data_collator)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=data_collator)

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# CNN model
class CNNTextClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_classes):
        super(CNNTextClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.conv1 = nn.Conv2d(1, 100, (3, embed_dim))
        self.conv2 = nn.Conv2d(1, 100, (4, embed_dim))
        self.conv3 = nn.Conv2d(1, 100, (5, embed_dim))
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(300, num_classes)

    def forward(self, input_ids):
        x = self.embedding(input_ids).unsqueeze(1)
        x1 = torch.relu(self.conv1(x)).squeeze(3)
        x2 = torch.relu(self.conv2(x)).squeeze(3)
        x3 = torch.relu(self.conv3(x)).squeeze(3)
        x1 = torch.max_pool1d(x1, x1.size(2)).squeeze(2)
        x2 = torch.max_pool1d(x2, x2.size(2)).squeeze(2)
        x3 = torch.max_pool1d(x3, x3.size(2)).squeeze(2)
        x = torch.cat((x1, x2, x3), 1)
        x = self.dropout(x)
        logits = self.fc(x)
        return logits

# Training and evaluation functions
def train(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        labels = batch['labels'].to(device)  # Use 'coarse_label' for labels
        outputs = model(input_ids)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1)
        correct += torch.sum(preds == labels).item()
    return total_loss / len(train_loader), correct / len(train_loader.dataset)

def evaluate(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)  # Use 'coarse_label' for labels
            outputs = model(input_ids)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += torch.sum(preds == labels).item()
    return total_loss / len(test_loader), correct / len(test_loader.dataset)

# Model initialization
embed_dim = 128
num_classes = 4  # TREC-6 has 6 classes
vocab_size = tokenizer.vocab_size
learning_rate = 1e-3
num_epochs = 15
model = CNNTextClassifier(vocab_size, embed_dim, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Metrics tracking
train_losses = []
test_losses = []
train_accuracies = []
test_accuracies = []

# Training loop
for epoch in range(num_epochs):
    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)
    test_loss, test_acc = evaluate(model, test_loader, criterion, device)

    train_losses.append(train_loss)
    test_losses.append(test_loss)
    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)

    print(f'Epoch {epoch+1}/{num_epochs}')
    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')
    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')

# Plot training and validation loss and accuracy
plt.figure(figsize=(12, 6))

# Loss Plot
plt.subplot(1, 2, 1)
plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss',marker='o', linestyle='-')
plt.plot(range(1, num_epochs+1), test_losses, label='Validation Loss',marker='o', linestyle='-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Train vs Validation Loss')
plt.legend()

# Accuracy Plot
plt.subplot(1, 2, 2)
plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy',marker='o', linestyle='-')
plt.plot(range(1, num_epochs+1), test_accuracies, label='Validation Accuracy',marker='o', linestyle='-')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Train vs Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()





"""#trec"""

# Load the dataset and tokenizer
dataset = load_dataset("trec", split="train")
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenization function (no need for padding here)
def tokenize(batch):
    return tokenizer(batch['text'], truncation=True, max_length=128)

# Apply tokenization
dataset = dataset.map(tokenize, batched=True)

# Split dataset using Hugging Face's built-in method
dataset = dataset.train_test_split(test_size=0.2, seed=42)

# Get the train and test sets
train_data = dataset['train']
test_data = dataset['test']

# Set format for train/test datasets
train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'coarse_label'])
test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'coarse_label'])

# Use DataCollatorWithPadding to handle dynamic padding within each batch
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Create DataLoaders
train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=data_collator)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=data_collator)

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# CNN model
class CNNTextClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_classes):
        super(CNNTextClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.conv1 = nn.Conv2d(1, 100, (3, embed_dim))
        self.conv2 = nn.Conv2d(1, 100, (4, embed_dim))
        self.conv3 = nn.Conv2d(1, 100, (5, embed_dim))
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(300, num_classes)

    def forward(self, input_ids):
        x = self.embedding(input_ids).unsqueeze(1)
        x1 = torch.relu(self.conv1(x)).squeeze(3)
        x2 = torch.relu(self.conv2(x)).squeeze(3)
        x3 = torch.relu(self.conv3(x)).squeeze(3)
        x1 = torch.max_pool1d(x1, x1.size(2)).squeeze(2)
        x2 = torch.max_pool1d(x2, x2.size(2)).squeeze(2)
        x3 = torch.max_pool1d(x3, x3.size(2)).squeeze(2)
        x = torch.cat((x1, x2, x3), 1)
        x = self.dropout(x)
        logits = self.fc(x)
        return logits

# Training and evaluation functions
def train(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        labels = batch['coarse_label'].to(device)  # Use 'coarse_label' for labels
        outputs = model(input_ids)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1)
        correct += torch.sum(preds == labels).item()
    return total_loss / len(train_loader), correct / len(train_loader.dataset)

def evaluate(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['coarse_label'].to(device)  # Use 'coarse_label' for labels
            outputs = model(input_ids)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += torch.sum(preds == labels).item()
    return total_loss / len(test_loader), correct / len(test_loader.dataset)

# Model initialization
embed_dim = 128
num_classes = 6  # TREC-6 has 6 classes
vocab_size = tokenizer.vocab_size
learning_rate = 1e-3
num_epochs = 15
model = CNNTextClassifier(vocab_size, embed_dim, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Metrics tracking
train_losses = []
test_losses = []
train_accuracies = []
test_accuracies = []

# Training loop
for epoch in range(num_epochs):
    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)
    test_loss, test_acc = evaluate(model, test_loader, criterion, device)

    train_losses.append(train_loss)
    test_losses.append(test_loss)
    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)

    print(f'Epoch {epoch+1}/{num_epochs}')
    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')
    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')

# Plot training and validation loss and accuracy
plt.figure(figsize=(12, 6))

# Loss Plot
plt.subplot(1, 2, 1)
plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss',marker='o', linestyle='-')
plt.plot(range(1, num_epochs+1), test_losses, label='Validation Loss',marker='o', linestyle='-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Train vs Validation Loss')
plt.legend()

# Accuracy Plot
plt.subplot(1, 2, 2)
plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy',marker='o', linestyle='-')
plt.plot(range(1, num_epochs+1), test_accuracies, label='Validation Accuracy',marker='o', linestyle='-')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Train vs Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluation with precision and recall calculation
from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_with_metrics(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['coarse_label'].to(device)
            outputs = model(input_ids)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += torch.sum(preds == labels).item()

            # Collect true and predicted labels
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    accuracy = correct / len(test_loader.dataset)
    avg_loss = total_loss / len(test_loader)

    # Calculate precision, recall, and F1 score
    precision = precision_score(y_true, y_pred, average='macro')
    recall = recall_score(y_true, y_pred, average='macro')
    f1 = f1_score(y_true, y_pred, average='macro')

    return avg_loss, accuracy, precision, recall, f1

# Model evaluation
test_loss, test_acc, precision, recall, f1 = evaluate_with_metrics(model, test_loader, criterion, device)

print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')
print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')

from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_with_metrics(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['coarse_label'].to(device)
            outputs = model(input_ids)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += torch.sum(preds == labels).item()

            # Collect true and predicted labels
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    accuracy = correct / len(test_loader.dataset)
    avg_loss = total_loss / len(test_loader)

    # Calculate macro precision, recall, and F1 score
    macro_precision = precision_score(y_true, y_pred, average='macro')
    macro_recall = recall_score(y_true, y_pred, average='macro')
    macro_f1 = f1_score(y_true, y_pred, average='macro')

    # Calculate weighted precision, recall, and F1 score
    weighted_precision = precision_score(y_true, y_pred, average='weighted')
    weighted_recall = recall_score(y_true, y_pred, average='weighted')
    weighted_f1 = f1_score(y_true, y_pred, average='weighted')

    # Generate confusion matrix (contingency table)
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Perform Chi-squared test
    chi2, p, dof, expected = chi2_contingency(conf_matrix)

    return

    return (avg_loss, accuracy,
            macro_precision, macro_recall, macro_f1,
            weighted_precision, weighted_recall, weighted_f1,
            chi2, p, dof, expected)

# Evaluate model
test_loss, test_acc, macro_precision, macro_recall, macro_f1, weighted_precision, weighted_recall, weighted_f1,chi2, p_value, dof, expected = evaluate_with_metrics(model, test_loader, criterion, device)

print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')
print(f'Macro Precision: {macro_precision:.4f}, Macro Recall: {macro_recall:.4f}, Macro F1 Score: {macro_f1:.4f}')
print(f'Weighted Precision: {weighted_precision:.4f}, Weighted Recall: {weighted_recall:.4f}, Weighted F1 Score: {weighted_f1:.4f}')


print(f"Chi-squared Statistic: {chi2:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies in Each Category:")
print(expected)

import numpy as np
from sklearn.metrics import confusion_matrix
from scipy.stats import chi2_contingency

def perform_chi_squared_test(y_true, y_pred):
    # Generate confusion matrix (contingency table)
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Perform Chi-squared test
    chi2, p, dof, expected = chi2_contingency(conf_matrix)

    return chi2, p, dof, expected

# Assuming y_true and y_pred are already defined
chi2, p_value, dof, expected = perform_chi_squared_test(y_true, y_pred)

print(f"Chi-squared Statistic: {chi2:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies in Each Category:")
print(expected)

!pip install torchsummary

from torchsummary import summary

# Assuming model is defined and on the correct device
summary(model, input_size=(1, 128), batch_size=-1)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assuming `y_true` are actual labels and `y_pred` are the predicted labels from the model
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='macro')
recall = recall_score(y_true, y_pred, average='macro')
f1 = f1_score(y_true, y_pred, average='macro')

print(f"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}")

from sklearn.metrics import confusion_matrix
from scipy.stats import chi2_contingency

cm = confusion_matrix(y_true, y_pred)

# Perform Chi-Square test
chi2, p, dof, ex = chi2_contingency(cm)

print(f"Chi-Square statistic: {chi2}, P-value: {p}")

# Interpretation: If p < 0.05, predictions are not independent from actual labels
if p < 0.05:
    print("Reject the null hypothesis, predictions are dependent on actual labels.")
else:
    print("Fail to reject the null hypothesis, predictions are independent of actual labels.")