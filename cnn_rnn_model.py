# -*- coding: utf-8 -*-
"""CNN-RNN Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n7gbT3BZ3p5qP1u37gaZG3JJT3OU8aj5
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# # Change directory
# import os
# os.chdir("drive/My Drive/contents/nlp")

# # Print out the current directory
# !pwd

"""Now let's read the data into a `pandas` dataframe and see what the dataset looks like.

The dataset has two columns. One column contains the reviews and the other column contains the sentiment label for the review. Since this tutorial is for topic modeling, we will not use the sentiment label column, so we removed it from the dataset.
"""

# Read in data
amz_review = pd.read_csv('/content/drive/MyDrive/Amazon_10k_preprocessed_230701_v1.csv')

# Drop te label
amz_review = amz_review.drop('labels', axis=1);

# Take a look at the data
amz_review.head()

"""`.info` helps us to get information about the dataset.

From the output, we can see that this data set has 1000 records, and no missing data. The 'review' column is the `object` type.
"""

df_MD = amz_review.loc[amz_review['label'] == 'Manufacturing_defect']
df_DP = amz_review.loc[amz_review['label'] == 'design_problem']
df_CS = amz_review.loc[amz_review['label'] == 'customer_service_issues']
df_NO = amz_review.loc[amz_review['label'] == 'no_defect_info']

# Get the dataset information
amz_review.info()

"""# Step 3: Text Data Preprocessing (Optional)

In step 3, we included some sample code for text data preprocessing.

Generally speaking, there is no need to preprocess the text data when using the python BERTopic model. However, since our dataset is a simple dataset, a lot of stopwords are picked to represent the topics.

Therefore, we removed stopwords and did lemmatization as data preprocessing. But please ignore this step if this is not an issue for you.
"""

# Remove stopwords
stopwords = nltk.corpus.stopwords.words('english')
print(f'There are {len(stopwords)} default stopwords. They are {stopwords}')

"""There are 179 default stopwords in the `nltk` library. We printed all the stopwords out to see what words are considered to be stopwords. `nltk` provides the option to add customized stopwords to the list.

Lemmatization refers to changing words to their base form.

After removing stopwords and lemmatizing the words we can see that the stopwords like `to` and `for` are removed, and the word like `conversations` is converted to `conversation`.
"""

# Remove stopwords
amz_review['review_without_stopwords'] = amz_review['review'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))

# Lemmatization
amz_review['review_lemmatized'] = amz_review['review_without_stopwords'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))

# Take a look at the data
amz_review.head()

"""# Step 4: Topic Modeling Using BERTopic"""

# Initiate UMAP
umap_model = UMAP(n_neighbors=15,
                  n_components=5,
                  min_dist=0.0,
                  metric='cosine',
                  random_state=100)

# Initiate BERTopic
topic_model = BERTopic(umap_model=umap_model, language="english", calculate_probabilities=True)

# Run BERTopic model
topics, probabilities = topic_model.fit_transform(amz_review['review'])

# Calculate the topic distribution
topic_counts = topic_model.get_topic_freq()

# Sort the topics based on count in descending order
topic_counts = topic_counts.sort_values("Count", ascending=False)

# Print the topic distribution
print(topic_counts)

"""# Step 5: Extract Topics From Topic Modeling

In step 5, we will extract topics from the BERTopic modeling results.

Using the attribute `get_topic_info()` on the topic model gives us the list of topics. We can see that the output gives us 31 rows in total.
* Topic -1 should be ignored. It indicates that the reviews are not assigned to any specific topic. The count for topic -1 is 322, meaning that there are 322 reviews as outliers that do not belong to any topic.
* Topic 0 to topic 29 are the 30 topics created for the reviews. It was ordered by the number of reviews in each topic, so topic 0 has the highest number of reviews.
* The `Name` column lists the top terms for each topic. For example, the top 4 terms for Topic 0 are sound, quality, volume, and audio, indicating that it is a topic related to sound quality.
"""

# Get the list of topics
topic_model.get_topic_info()

df = topic_model.get_topic_info()[1:]

df.plot.bar(y='Count', rot=0)
# df['Count'].plot.kde()

"""If more than 4 terms are needed for a topic, we can use `get_topic` and pass in the topic number. For example, `get_topic(0)` gives us the top 10 terms for topic 0 and their relative importance."""

# Get top 10 terms for a topic
topic_model.get_topic(0)

"""We can visualize the top keywords using a bar chart. `top_n_topics=12` means that we will create bar charts for the top 12 topics. The length of the bar represents the score of the keyword. A longer bar means higher importance for the topic."""

# Visualize top topic keywords
topic_model.visualize_barchart(top_n_topics=10)

"""Another view for keyword importance is the "Term score decline per topic" chart. It's a line chart with the term rank being the x-axis and the c-TF-IDF score on the y-axis.

There are a total of 31 lines, one line for each topic. Hovering over the line shows the term score information.
"""

# Visualize term rank decrease
topic_model.visualize_term_rank()

"""# Step 6: Topic Similarities

In step 6, we will analyze the relationship between the topics generated by the topic model.

We will use three visualizations to study how the topics are related to each other. The three methods are intertopic distance map, the hierarchical clustering of topics, and the topic similarity matrix.

Intertopic distance map measures the distance between topics. Similar topics are closer to each other, and very different topics are far from each other. From the visualization, we can see that there are five topic groups for all the topics. Topics with similar semantic meanings are in the same topic group.

The size of the circle represents the number of documents in the topics, and larger circles mean that more reviews belong to the topic.
"""

# Visualize intertopic distance
topic_model.visualize_topics()

"""Another way to see how the topics are connected is through a hierarchical clustering graph. We can control the number of topics in the graph by the `top_n_topics` parameter.

In this example, the top 10 topics are included in the hierarchical graph. We can see that the sound quality topic is closely connected to the headset topic, and both of them are connected to the earpiece comfortable topic.
"""

from bertopic import BERTopic
from sklearn.metrics.pairwise import cosine_similarity
from itertools import combinations

def calculate_coherence(topics, embeddings):
    num_topics = len(topics)
    total_coherence = 0.0
    pair_count = 0

    for i in range(num_topics):
        topic = topics[i]
        topic_embeddings = embeddings[i]

        # Compute pairwise cosine similarity between words in the topic
        similarities = cosine_similarity(topic_embeddings)

        # Calculate average pairwise cosine similarity
        coherence = 0.0
        for pair in combinations(range(len(topic)), 2):
            coherence += similarities[pair[0], pair[1]]
            pair_count += 1

        if pair_count > 0:
            coherence /= pair_count
            total_coherence += coherence

    if num_topics > 0:
        total_coherence /= num_topics

    return total_coherence

# Get the topics and embeddings
topics_with_embeddings = topic_model.get_topics()

# Separate the topics and embeddings
topics = [topic for topic, _ in topics_with_embeddings]
embeddings = [embedding for _, embedding in topics_with_embeddings]

# Calculate coherence score
coherence_score = calculate_coherence(topics_with_embeddings)
print("Coherence Score:", coherence_score)

coherence_model = CoherenceModel(topics=topics_with_embeddings,
                                 coherence='c_v')
coherence = coherence_model.get_coherence()

# Visualize connections between topics using hierachical clustering
topic_model.visualize_hierarchy(top_n_topics=10)

"""Heatmap can also be used to analyze the similarities between topics. The similarity score ranges from 0 to 1. A value close to 1 represents a higher similarity between the two topics, which is represented by darker blue color."""

# Visualize similarity using heatmap
topic_model.visualize_heatmap()

# Get topics by group
topics_per_class = topic_model.topics_per_class(df_CS['review'], classes=df_CS['label'])

# Visualize topics by group
topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=20)

"""# Step 7: Topic Model Predicted Probabilities

In step 7, we will talk about how to use BERTopic model to get predicted probabilities.

The topic prediction for a document is based on the predicted probabilities of the document belonging to each topic. The topic with the highest probability is the predicted topic. This probability represents how confident we are about finding the topic in the document.

We can visualize the probabilities using `visualize_distribution`, and pass in the document index. `visualize_distribution` has the default probability threshold of 0.015, so only the topic with a probability greater than 0.015 will be included.
"""

# Visualize probability distribution
topic_model.visualize_distribution(topic_model.probabilities_[0], min_probability=0.015)

"""If you would like to save the visualization as a separate html file, we can save the chart into a variable and use `write_html` to write the chart into a file."""

# Save the chart to a variable
chart = topic_model.visualize_distribution(topic_model.probabilities_[0])

# Write the chart as a html file
chart.write_html("amz_review_topic_probability_distribution.html")

"""The topic probability distribution for the first review in the dataset shows that topic 7 has the highest probability, so topic 7 is the predicted topic.

The first review is "So there is no way for me to plug it in here in the US unless I go by a converter.", and the topic of plugging a charger is pretty relevant.
"""

# Check the content for the first review
amz_review['review'][0]

"""We can also get the predicted probability for all topics using the code below."""

# Get probabilities for all topics
topic_model.probabilities_[0]

"""We can see that there are 30 probability values, one for each topic. The index 7 has the highest value, indicating that topic 7 is the predicted topic.

# Step 8: Topic Model In-sample Predictions

In step 8, we will talk about how to make in-sample predictions using the topic model.

BERTopic model can output the predicted topic for each review in the dataset.

Using `.topics_`, we save the predicted topics in a list and then save it as a column in the review dataset.
"""

# Get the topic predictions
topic_prediction = topic_model.topics_[:]

# Save the predictions in the dataframe
amz_review['topic_prediction'] = topic_prediction

# Take a look at the data
amz_review.head()

"""# Step 9: Topic Model Predictions on New Data

In step 9, we will talk about how to use the BERTopic model to make predictions on new reviews.

Let's say there is a new review "I like the new headphone. Its sound quality is great.", and we would like to automatically predict the topic for this review.
* Firstly, let's decide the number of topics to include in the prediction.
 * If we would like to assign only one topic to the document, then the number of topics should be 1.  
 * If we would like to assign multiple topics to the document, then the number of topics should be greater than 1. Here we are getting the top 3 topics that are most relevant to the new review.
* After that, we pass the new review and the number of topics to the `find_topics` method. This gives us the topic number and the similarity value.
* Finally, the results are printed. The top 3 similar topics for the new review are topic 1, topic 0, and topic 2. The similarities are 0.43, 0.34, and 0.30.
"""

# New data for the review
new_review = "I like the new headphone. Its sound quality is great."

# Find topics
num_of_topics = 3
similar_topics, similarity = topic_model.find_topics(new_review, top_n=num_of_topics);

# Print results
print(f'The top {num_of_topics} similar topics are {similar_topics}, and the similarities are {np.round(similarity,2)}')

"""To verify if the assigned topics are a good fit for the new review, let's get the top keywords for the top 3 topics using the `get_topic` method."""

# Print the top keywords for the top similar topics
for i in range(num_of_topics):
  print(f'The top keywords for topic {similar_topics[i]} are:')
  print(topic_model.get_topic(similar_topics[i]))

"""We can see that topic 1 is about headsets and topic 0 is about sound quality. Both topics are a good fit for the new review. Topic 2 is about the earpiece, which is similar to the headset. From this example, we can see that the BERTopic model made good predictions on the new document.

# Step 10: Save and Load Topic Models

In step 10, we will talk about how to save and load BERTopic models.

The trained BERTopic model and its settings can be saved using `.save`. UMAP and HDBSCAN are saved, but the documents and embeddings are not saved.

We can use `.load` to load the saved BERTopic model.
"""

# Save the topic model
topic_model.save("amz_review_topic_model")

# Load the topic model
my_model = BERTopic.load("amz_review_topic_model")